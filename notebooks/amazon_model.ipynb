{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Amazon Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook will build and test several models abilities to predict whether a review\n",
    "was helpful or not\n",
    "\n",
    "The problem is defined as follows: \n",
    "It is a binary classification problemm, essentially, the model will predict 1 if the review (and other features) \n",
    "associated to that product was helpful, 0 otherwise.\n",
    "\n",
    "It will be trained on select numerical features which were generated during the data preprocessing segment\n",
    "(refer to the amazon_data.ipynb notebook). The label to train the model to predict on will be identified using the\n",
    "vote column values:\n",
    "    If there is a vote > 0 for a particular review for a product, then the label will be 1 (essentially, someone found it\n",
    "    useful of a review). Otherwise the label will be 0, essentially, no one found it as a useful review.\n",
    "    \n",
    "For a first step, I will explore the Logistic Regression and Random Forest Classifier models for this tasks. They were chosen \n",
    "because of their overall popularity and performance for these types of tasks. I will assess the accuracy and the accuracy on a \n",
    "validation set associated to both models and choose the best model for the rest API. \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "base_path = os.path.expanduser('~') + '/'\n",
    "data_path = 'Downloads/video_game_data_preprocessed.csv'\n",
    "\n",
    "features_cols = [\n",
    "    'price', 'overall', 'main_cat', 'verified', 'valid_image', 'review_year', 'time_since_review',\n",
    "    'review_wc', 'summary_wc', 'title_wc', 'description_count',\n",
    "    'similar_item_present', 'recommended_item_counts', 'count_also_bought',\n",
    "    'image_present', 'best_rank', 'vote'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(base_path + data_path, low_memory = False, nrows=250000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = d[features_cols].dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(row):\n",
    "    '''\n",
    "    This function will return a label of 1 if there is a vote for a review which is greater than 0,\n",
    "    otherwise it will return a 0.\n",
    "        1 : The review was helpfull \n",
    "        2 : The review was not helpfull\n",
    "    '''\n",
    "    \n",
    "    if int(row) > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.5 ms, sys: 2.76 ms, total: 43.2 ms\n",
      "Wall time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "%time feat_df['label'] = feat_df['vote'].apply(lambda x : generate_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cats = pd.get_dummies(feat_df.main_cat, prefix='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = pd.concat([encoded_cats, feat_df], axis = 1).drop(columns = ['main_cat', 'vote'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = feat_df.sample(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = feat_df[~feat_df.index.isin(validation_df.index.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feat_df.drop(columns = ['label']).values\n",
    "y = feat_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X ,y ,test_size = 0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 16.8 ms, total: 1.53 s\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lr = LogisticRegression(max_iter = 1000)  \n",
    "reg = lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = reg.predict(x_train)  \n",
    "pred_test = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89      6735\n",
      "           1       0.65      0.32      0.43      1932\n",
      "\n",
      "    accuracy                           0.81      8667\n",
      "   macro avg       0.74      0.64      0.66      8667\n",
      "weighted avg       0.79      0.81      0.79      8667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_val = validation_df.copy()\n",
    "lr_feats = lr_val.drop(columns = ['label']).values\n",
    "lr_val['pred'] = reg.predict(lr_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.608\n"
     ]
    }
   ],
   "source": [
    "true_positives = lr_val[lr_val['label'] == lr_val['pred']].shape[0]\n",
    "total_sample = lr_val.shape[0]\n",
    "accuracy = true_positives / total_sample\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 21.3 ms, total: 1.75 s\n",
      "Wall time: 1.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier()\n",
    "rf_clf = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train_rf = rf_clf.predict(x_train)  \n",
    "pred_test_rf = rf_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92      6735\n",
      "           1       0.76      0.61      0.68      1932\n",
      "\n",
      "    accuracy                           0.87      8667\n",
      "   macro avg       0.83      0.78      0.80      8667\n",
      "weighted avg       0.86      0.87      0.87      8667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_val = validation_df.copy()\n",
    "rf_feats = rf_val.drop(columns = ['label']).values\n",
    "rf_val['pred'] = rf_clf.predict(rf_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.64\n"
     ]
    }
   ],
   "source": [
    "true_positives = rf_val[rf_val['label'] == rf_val['pred']].shape[0]\n",
    "total_sample = rf_val.shape[0]\n",
    "accuracy = true_positives / total_sample\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/random_forest_clf_model_vg.sav'\n",
    "pickle.dump(rf_clf, open(model_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
